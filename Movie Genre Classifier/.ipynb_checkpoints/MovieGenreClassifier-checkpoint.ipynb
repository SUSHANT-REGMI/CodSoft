{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae5a1e2-ef3a-4ddc-84b5-e532bcf09801",
   "metadata": {},
   "source": [
    "# Task: MOVIE GENRE CLASSIFICATION\n",
    "    Create a machine learning model that can predict the genre of a\n",
    "    movie based on its plot summary or other textual information. You\n",
    "    can use techniques like TF-IDF or word embeddings with classifiers\n",
    "    such as Naive Bayes, Logistic Regression, or Support Vector\n",
    "    Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595aac1-0b79-4c61-baf1-5eefcc0b947a",
   "metadata": {},
   "source": [
    "## Methodologies\n",
    "\n",
    "###    1. Data Collection\n",
    "###    2. Data Cleaning and Preprocessing\n",
    "###    3. Data Visualization\n",
    "###    4. Feature Engineering\n",
    "###    5. Model Selection\n",
    "###    6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b90354-d58d-4edd-9bf7-d9db2c694008",
   "metadata": {},
   "source": [
    "## Data Collection: Data was collected from https://www.kaggle.com/code/dhruvtibarewal/movie-genre-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62062b37-acef-4b98-948c-343c87d5a4f7",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f960f8-7709-4acd-b591-5f9183a4444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0f39ea-dcea-4949-b2cb-bdbb0566d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"C:/Users/susha/Downloads/archive (7)/Genre Classification Dataset/train_data.txt\", delimiter=':::', names = ['Sno', 'Name', 'Genre', 'Description'] ,engine='python')\n",
    "test_data = pd.read_csv(\"C:/Users/susha/Downloads/archive (7)/Genre Classification Dataset/test_data.txt\", delimiter = ':::', names = ['Sno', 'Name', 'Description'], engine='python')\n",
    "test_data_solution = pd.read_csv(\"C:/Users/susha/Downloads/archive (7)/Genre Classification Dataset/test_data_solution.txt\", delimiter=':::', names = ['Sno', 'Name', 'Genre', 'Description'] ,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0a6135-297d-4720-9ee1-a346d43af644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54195</th>\n",
       "      <td>54196</td>\n",
       "      <td>\"Tales of Light &amp; Dark\" (2013)</td>\n",
       "      <td>horror</td>\n",
       "      <td>Covering multiple genres, Tales of Light &amp; Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54196</th>\n",
       "      <td>54197</td>\n",
       "      <td>Der letzte Mohikaner (1965)</td>\n",
       "      <td>western</td>\n",
       "      <td>As Alice and Cora Munro attempt to find their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54197</th>\n",
       "      <td>54198</td>\n",
       "      <td>Oliver Twink (2007)</td>\n",
       "      <td>adult</td>\n",
       "      <td>A movie 169 years in the making. Oliver Twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54198</th>\n",
       "      <td>54199</td>\n",
       "      <td>Slipstream (1973)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Popular, but mysterious rock D.J Mike Mallard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54199</th>\n",
       "      <td>54200</td>\n",
       "      <td>Curitiba Zero Grau (2010)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Curitiba is a city in movement, with rhythms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                              Name      Genre  \\\n",
       "54195  54196   \"Tales of Light & Dark\" (2013)     horror    \n",
       "54196  54197      Der letzte Mohikaner (1965)    western    \n",
       "54197  54198              Oliver Twink (2007)      adult    \n",
       "54198  54199                Slipstream (1973)      drama    \n",
       "54199  54200        Curitiba Zero Grau (2010)      drama    \n",
       "\n",
       "                                             Description  \n",
       "54195   Covering multiple genres, Tales of Light & Da...  \n",
       "54196   As Alice and Cora Munro attempt to find their...  \n",
       "54197   A movie 169 years in the making. Oliver Twist...  \n",
       "54198   Popular, but mysterious rock D.J Mike Mallard...  \n",
       "54199   Curitiba is a city in movement, with rhythms ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n",
    "test_data.head()\n",
    "test_data_solution.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1916cef-9d81-4ae2-bb3a-830d35488e35",
   "metadata": {},
   "source": [
    "#### Looking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6fa0ec-cee8-40f4-ae18-0528e7ee7ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54214 entries, 0 to 54213\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Sno          54214 non-null  int64 \n",
      " 1   Name         54214 non-null  object\n",
      " 2   Genre        54214 non-null  object\n",
      " 3   Description  54214 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.7+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54200 entries, 0 to 54199\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Sno          54200 non-null  int64 \n",
      " 1   Name         54200 non-null  object\n",
      " 2   Description  54200 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54200 entries, 0 to 54199\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Sno          54200 non-null  int64 \n",
      " 1   Name         54200 non-null  object\n",
      " 2   Genre        54200 non-null  object\n",
      " 3   Description  54200 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#looking for null values\n",
    "\n",
    "train_data.info()\n",
    "print('\\n')\n",
    "test_data.info()\n",
    "print('\\n')\n",
    "test_data_solution.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3003746d-cc22-4e29-b1f3-6a682f34e941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sno            0\n",
       "Name           0\n",
       "Genre          0\n",
       "Description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()\n",
    "\n",
    "test_data.isna().sum()\n",
    "\n",
    "test_data_solution.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab90ca7-0ff2-4d7f-a19f-98f00a92da96",
   "metadata": {},
   "source": [
    "#### Looking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00534954-04c3-4232-ac17-bddda3abda28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated().sum()\n",
    "test_data.duplicated().sum()\n",
    "test_data_solution.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979b13b-1d5f-43a8-8304-44fd949d9f5b",
   "metadata": {},
   "source": [
    "# Data Cleaning is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af0f8-e6ca-4f95-8289-0a73249ccbd0",
   "metadata": {},
   "source": [
    "## Futher preprocessing\n",
    "#### The dataset here is almost 50% training  and 50% testing. \n",
    "#### This is far from the optimal ratio that will yield in a better working model. \n",
    "#### So we will be splitting the dataset in the format 70-15-15 for training, validation and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc55877-ca4e-4416-b98a-60ff119a90be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfa24e3-f715-41d8-b156-3e3b211d83a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54214\n",
      "         Sno                                    Name          Genre  \\\n",
      "0      54216                   Edgar's Lunch (1998)       thriller    \n",
      "1      54217               La guerra de papá (1977)         comedy    \n",
      "2      54218            Off the Beaten Track (2010)    documentary    \n",
      "3      54219                 Meu Amigo Hindu (2015)          drama    \n",
      "4      54220                      Er nu zhai (1955)          drama    \n",
      "...      ...                                     ...            ...   \n",
      "37695  91911                    Fully Loaded (2011)         comedy    \n",
      "37696  91912                    Tenebrae Lux (2014)         sci-fi    \n",
      "37697  91913                   Mexican Dance (1898)          short    \n",
      "37698  91914   Das Lied von den zwei Pferden (2009)    documentary    \n",
      "37699  91915                  Doin' It Again (2012)    documentary    \n",
      "\n",
      "                                             Description  \n",
      "0       L.R. Brane loves his life - his car, his apar...  \n",
      "1       Spain, March 1964: Quico is a very naughty ch...  \n",
      "2       One year in the life of Albin and his family ...  \n",
      "3       His father has died, he hasn't spoken with hi...  \n",
      "4       Before he was known internationally as a mart...  \n",
      "...                                                  ...  \n",
      "37695   On a rare evening out, two feisty single moms...  \n",
      "37696   A lone traveler with the ability to cross bet...  \n",
      "37697   \"Another well-known dancer with a national re...  \n",
      "37698   A promise, an old, destroyed horse head violi...  \n",
      "37699   50th Anniversary - 1st album of new material ...  \n",
      "\n",
      "[37700 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susha\\AppData\\Local\\Temp\\ipykernel_8860\\1078671139.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data = train_data.append(rows_to_append)\n"
     ]
    }
   ],
   "source": [
    "#code below adds the first 37700 data from test_data_solution into the train datasets and removes those respective data from itself and test_data\n",
    "\n",
    "last_sno = train_data['Sno'].max()\n",
    "print(last_sno)\n",
    "\n",
    "rows_to_append = test_data_solution.head(37700).copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "rows_to_append.loc[:, 'Sno'] += last_sno + 1  # Use .loc to modify the DataFrame safely\n",
    "\n",
    "print(rows_to_append)\n",
    "\n",
    "\n",
    "train_data = train_data.append(rows_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae1fa90-097e-4b7f-80b4-adf6b9452450",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(rows_to_append.index)\n",
    "test_data_solution = test_data_solution.drop(rows_to_append.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5018a4f5-8067-421d-b0c1-7de13c6d889b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oscar et la dame rose (2009)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Listening in to a conversation between his do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Cupid (1997)</td>\n",
       "      <td>thriller</td>\n",
       "      <td>A brother and sister with a past incestuous r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Young, Wild and Wonderful (1980)</td>\n",
       "      <td>adult</td>\n",
       "      <td>As the bus empties the students for their fie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Secret Sin (1915)</td>\n",
       "      <td>drama</td>\n",
       "      <td>To help their unemployed father make ends mee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Unrecovered (2007)</td>\n",
       "      <td>drama</td>\n",
       "      <td>The film's title refers not only to the un-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37695</th>\n",
       "      <td>91911</td>\n",
       "      <td>Fully Loaded (2011)</td>\n",
       "      <td>comedy</td>\n",
       "      <td>On a rare evening out, two feisty single moms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37696</th>\n",
       "      <td>91912</td>\n",
       "      <td>Tenebrae Lux (2014)</td>\n",
       "      <td>sci-fi</td>\n",
       "      <td>A lone traveler with the ability to cross bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37697</th>\n",
       "      <td>91913</td>\n",
       "      <td>Mexican Dance (1898)</td>\n",
       "      <td>short</td>\n",
       "      <td>\"Another well-known dancer with a national re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37698</th>\n",
       "      <td>91914</td>\n",
       "      <td>Das Lied von den zwei Pferden (2009)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>A promise, an old, destroyed horse head violi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37699</th>\n",
       "      <td>91915</td>\n",
       "      <td>Doin' It Again (2012)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>50th Anniversary - 1st album of new material ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91914 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                                    Name          Genre  \\\n",
       "0          1           Oscar et la dame rose (2009)          drama    \n",
       "1          2                           Cupid (1997)       thriller    \n",
       "2          3       Young, Wild and Wonderful (1980)          adult    \n",
       "3          4                  The Secret Sin (1915)          drama    \n",
       "4          5                 The Unrecovered (2007)          drama    \n",
       "...      ...                                     ...            ...   \n",
       "37695  91911                    Fully Loaded (2011)         comedy    \n",
       "37696  91912                    Tenebrae Lux (2014)         sci-fi    \n",
       "37697  91913                   Mexican Dance (1898)          short    \n",
       "37698  91914   Das Lied von den zwei Pferden (2009)    documentary    \n",
       "37699  91915                  Doin' It Again (2012)    documentary    \n",
       "\n",
       "                                             Description  \n",
       "0       Listening in to a conversation between his do...  \n",
       "1       A brother and sister with a past incestuous r...  \n",
       "2       As the bus empties the students for their fie...  \n",
       "3       To help their unemployed father make ends mee...  \n",
       "4       The film's title refers not only to the un-re...  \n",
       "...                                                  ...  \n",
       "37695   On a rare evening out, two feisty single moms...  \n",
       "37696   A lone traveler with the ability to cross bet...  \n",
       "37697   \"Another well-known dancer with a national re...  \n",
       "37698   A promise, an old, destroyed horse head violi...  \n",
       "37699   50th Anniversary - 1st album of new material ...  \n",
       "\n",
       "[91914 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cff59315-b916-40ad-8c6d-5857ab662ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37700</th>\n",
       "      <td>37701</td>\n",
       "      <td>My Lips Betray (1933)</td>\n",
       "      <td>In a make-believe, mittleuropean kingdom, a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37701</th>\n",
       "      <td>37702</td>\n",
       "      <td>The Koreas (2016)</td>\n",
       "      <td>At the end of World War II, Korea was divided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37702</th>\n",
       "      <td>37703</td>\n",
       "      <td>Come Together (2016)</td>\n",
       "      <td>Colombia is coming out of a period in their h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37703</th>\n",
       "      <td>37704</td>\n",
       "      <td>With Honors Denied (2003)</td>\n",
       "      <td>Japanese bombs hit Pearl Harbor on a Sunday. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37704</th>\n",
       "      <td>37705</td>\n",
       "      <td>\"Connect with English\" (2007)</td>\n",
       "      <td>Connect with English is a series that brings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54195</th>\n",
       "      <td>54196</td>\n",
       "      <td>\"Tales of Light &amp; Dark\" (2013)</td>\n",
       "      <td>Covering multiple genres, Tales of Light &amp; Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54196</th>\n",
       "      <td>54197</td>\n",
       "      <td>Der letzte Mohikaner (1965)</td>\n",
       "      <td>As Alice and Cora Munro attempt to find their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54197</th>\n",
       "      <td>54198</td>\n",
       "      <td>Oliver Twink (2007)</td>\n",
       "      <td>A movie 169 years in the making. Oliver Twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54198</th>\n",
       "      <td>54199</td>\n",
       "      <td>Slipstream (1973)</td>\n",
       "      <td>Popular, but mysterious rock D.J Mike Mallard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54199</th>\n",
       "      <td>54200</td>\n",
       "      <td>Curitiba Zero Grau (2010)</td>\n",
       "      <td>Curitiba is a city in movement, with rhythms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                              Name  \\\n",
       "37700  37701            My Lips Betray (1933)    \n",
       "37701  37702                The Koreas (2016)    \n",
       "37702  37703             Come Together (2016)    \n",
       "37703  37704        With Honors Denied (2003)    \n",
       "37704  37705    \"Connect with English\" (2007)    \n",
       "...      ...                               ...   \n",
       "54195  54196   \"Tales of Light & Dark\" (2013)    \n",
       "54196  54197      Der letzte Mohikaner (1965)    \n",
       "54197  54198              Oliver Twink (2007)    \n",
       "54198  54199                Slipstream (1973)    \n",
       "54199  54200        Curitiba Zero Grau (2010)    \n",
       "\n",
       "                                             Description  \n",
       "37700   In a make-believe, mittleuropean kingdom, a v...  \n",
       "37701   At the end of World War II, Korea was divided...  \n",
       "37702   Colombia is coming out of a period in their h...  \n",
       "37703   Japanese bombs hit Pearl Harbor on a Sunday. ...  \n",
       "37704   Connect with English is a series that brings ...  \n",
       "...                                                  ...  \n",
       "54195   Covering multiple genres, Tales of Light & Da...  \n",
       "54196   As Alice and Cora Munro attempt to find their...  \n",
       "54197   A movie 169 years in the making. Oliver Twist...  \n",
       "54198   Popular, but mysterious rock D.J Mike Mallard...  \n",
       "54199   Curitiba is a city in movement, with rhythms ...  \n",
       "\n",
       "[16500 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adf68f7-9fea-444a-93c9-5ee68c03c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37700</th>\n",
       "      <td>37701</td>\n",
       "      <td>My Lips Betray (1933)</td>\n",
       "      <td>musical</td>\n",
       "      <td>In a make-believe, mittleuropean kingdom, a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37701</th>\n",
       "      <td>37702</td>\n",
       "      <td>The Koreas (2016)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>At the end of World War II, Korea was divided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37702</th>\n",
       "      <td>37703</td>\n",
       "      <td>Come Together (2016)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>Colombia is coming out of a period in their h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37703</th>\n",
       "      <td>37704</td>\n",
       "      <td>With Honors Denied (2003)</td>\n",
       "      <td>short</td>\n",
       "      <td>Japanese bombs hit Pearl Harbor on a Sunday. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37704</th>\n",
       "      <td>37705</td>\n",
       "      <td>\"Connect with English\" (2007)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Connect with English is a series that brings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54195</th>\n",
       "      <td>54196</td>\n",
       "      <td>\"Tales of Light &amp; Dark\" (2013)</td>\n",
       "      <td>horror</td>\n",
       "      <td>Covering multiple genres, Tales of Light &amp; Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54196</th>\n",
       "      <td>54197</td>\n",
       "      <td>Der letzte Mohikaner (1965)</td>\n",
       "      <td>western</td>\n",
       "      <td>As Alice and Cora Munro attempt to find their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54197</th>\n",
       "      <td>54198</td>\n",
       "      <td>Oliver Twink (2007)</td>\n",
       "      <td>adult</td>\n",
       "      <td>A movie 169 years in the making. Oliver Twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54198</th>\n",
       "      <td>54199</td>\n",
       "      <td>Slipstream (1973)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Popular, but mysterious rock D.J Mike Mallard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54199</th>\n",
       "      <td>54200</td>\n",
       "      <td>Curitiba Zero Grau (2010)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Curitiba is a city in movement, with rhythms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                              Name          Genre  \\\n",
       "37700  37701            My Lips Betray (1933)        musical    \n",
       "37701  37702                The Koreas (2016)    documentary    \n",
       "37702  37703             Come Together (2016)    documentary    \n",
       "37703  37704        With Honors Denied (2003)          short    \n",
       "37704  37705    \"Connect with English\" (2007)          drama    \n",
       "...      ...                               ...            ...   \n",
       "54195  54196   \"Tales of Light & Dark\" (2013)         horror    \n",
       "54196  54197      Der letzte Mohikaner (1965)        western    \n",
       "54197  54198              Oliver Twink (2007)          adult    \n",
       "54198  54199                Slipstream (1973)          drama    \n",
       "54199  54200        Curitiba Zero Grau (2010)          drama    \n",
       "\n",
       "                                             Description  \n",
       "37700   In a make-believe, mittleuropean kingdom, a v...  \n",
       "37701   At the end of World War II, Korea was divided...  \n",
       "37702   Colombia is coming out of a period in their h...  \n",
       "37703   Japanese bombs hit Pearl Harbor on a Sunday. ...  \n",
       "37704   Connect with English is a series that brings ...  \n",
       "...                                                  ...  \n",
       "54195   Covering multiple genres, Tales of Light & Da...  \n",
       "54196   As Alice and Cora Munro attempt to find their...  \n",
       "54197   A movie 169 years in the making. Oliver Twist...  \n",
       "54198   Popular, but mysterious rock D.J Mike Mallard...  \n",
       "54199   Curitiba is a city in movement, with rhythms ...  \n",
       "\n",
       "[16500 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876057c3-28a6-4c16-a64e-d9adcc056ad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1541890828.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\susha\\AppData\\Local\\Temp\\ipykernel_8860\\1541890828.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    train_data.\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f958c0-0ca0-4f14-90db-18040dd0acd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sno                                                        54202\n",
       "Name                                        Singing Guns (1950) \n",
       "Genre                                                   western \n",
       "Description     Rhiannon, an outlaw who regularly robs gold f...\n",
       "Name: 54201, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[54201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e37779-cf62-4918-b05c-09617372e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_solution.reset_index(drop=True, inplace=True)\n",
    "test_data_solution.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e5f03c-66a3-44f2-aa61-7d7afca3c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sno', 'Name', 'Genre', 'Description'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_solution.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4742ec32-7511-43c4-aabf-20f59eea41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.reset_index(drop=True, inplace=True)\n",
    "test_data.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b403eef-4691-46d0-ba50-cc84632516e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37701</td>\n",
       "      <td>My Lips Betray (1933)</td>\n",
       "      <td>In a make-believe, mittleuropean kingdom, a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37702</td>\n",
       "      <td>The Koreas (2016)</td>\n",
       "      <td>At the end of World War II, Korea was divided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37703</td>\n",
       "      <td>Come Together (2016)</td>\n",
       "      <td>Colombia is coming out of a period in their h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37704</td>\n",
       "      <td>With Honors Denied (2003)</td>\n",
       "      <td>Japanese bombs hit Pearl Harbor on a Sunday. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37705</td>\n",
       "      <td>\"Connect with English\" (2007)</td>\n",
       "      <td>Connect with English is a series that brings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16496</th>\n",
       "      <td>54196</td>\n",
       "      <td>\"Tales of Light &amp; Dark\" (2013)</td>\n",
       "      <td>Covering multiple genres, Tales of Light &amp; Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16497</th>\n",
       "      <td>54197</td>\n",
       "      <td>Der letzte Mohikaner (1965)</td>\n",
       "      <td>As Alice and Cora Munro attempt to find their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16498</th>\n",
       "      <td>54198</td>\n",
       "      <td>Oliver Twink (2007)</td>\n",
       "      <td>A movie 169 years in the making. Oliver Twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16499</th>\n",
       "      <td>54199</td>\n",
       "      <td>Slipstream (1973)</td>\n",
       "      <td>Popular, but mysterious rock D.J Mike Mallard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16500</th>\n",
       "      <td>54200</td>\n",
       "      <td>Curitiba Zero Grau (2010)</td>\n",
       "      <td>Curitiba is a city in movement, with rhythms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                              Name  \\\n",
       "1      37701            My Lips Betray (1933)    \n",
       "2      37702                The Koreas (2016)    \n",
       "3      37703             Come Together (2016)    \n",
       "4      37704        With Honors Denied (2003)    \n",
       "5      37705    \"Connect with English\" (2007)    \n",
       "...      ...                               ...   \n",
       "16496  54196   \"Tales of Light & Dark\" (2013)    \n",
       "16497  54197      Der letzte Mohikaner (1965)    \n",
       "16498  54198              Oliver Twink (2007)    \n",
       "16499  54199                Slipstream (1973)    \n",
       "16500  54200        Curitiba Zero Grau (2010)    \n",
       "\n",
       "                                             Description  \n",
       "1       In a make-believe, mittleuropean kingdom, a v...  \n",
       "2       At the end of World War II, Korea was divided...  \n",
       "3       Colombia is coming out of a period in their h...  \n",
       "4       Japanese bombs hit Pearl Harbor on a Sunday. ...  \n",
       "5       Connect with English is a series that brings ...  \n",
       "...                                                  ...  \n",
       "16496   Covering multiple genres, Tales of Light & Da...  \n",
       "16497   As Alice and Cora Munro attempt to find their...  \n",
       "16498   A movie 169 years in the making. Oliver Twist...  \n",
       "16499   Popular, but mysterious rock D.J Mike Mallard...  \n",
       "16500   Curitiba is a city in movement, with rhythms ...  \n",
       "\n",
       "[16500 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ab1475-8e7a-4f70-aa2a-0c00cad6491a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sno</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37701</td>\n",
       "      <td>My Lips Betray (1933)</td>\n",
       "      <td>musical</td>\n",
       "      <td>In a make-believe, mittleuropean kingdom, a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37702</td>\n",
       "      <td>The Koreas (2016)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>At the end of World War II, Korea was divided...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37703</td>\n",
       "      <td>Come Together (2016)</td>\n",
       "      <td>documentary</td>\n",
       "      <td>Colombia is coming out of a period in their h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37704</td>\n",
       "      <td>With Honors Denied (2003)</td>\n",
       "      <td>short</td>\n",
       "      <td>Japanese bombs hit Pearl Harbor on a Sunday. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37705</td>\n",
       "      <td>\"Connect with English\" (2007)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Connect with English is a series that brings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16496</th>\n",
       "      <td>54196</td>\n",
       "      <td>\"Tales of Light &amp; Dark\" (2013)</td>\n",
       "      <td>horror</td>\n",
       "      <td>Covering multiple genres, Tales of Light &amp; Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16497</th>\n",
       "      <td>54197</td>\n",
       "      <td>Der letzte Mohikaner (1965)</td>\n",
       "      <td>western</td>\n",
       "      <td>As Alice and Cora Munro attempt to find their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16498</th>\n",
       "      <td>54198</td>\n",
       "      <td>Oliver Twink (2007)</td>\n",
       "      <td>adult</td>\n",
       "      <td>A movie 169 years in the making. Oliver Twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16499</th>\n",
       "      <td>54199</td>\n",
       "      <td>Slipstream (1973)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Popular, but mysterious rock D.J Mike Mallard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16500</th>\n",
       "      <td>54200</td>\n",
       "      <td>Curitiba Zero Grau (2010)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Curitiba is a city in movement, with rhythms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sno                              Name          Genre  \\\n",
       "1      37701            My Lips Betray (1933)        musical    \n",
       "2      37702                The Koreas (2016)    documentary    \n",
       "3      37703             Come Together (2016)    documentary    \n",
       "4      37704        With Honors Denied (2003)          short    \n",
       "5      37705    \"Connect with English\" (2007)          drama    \n",
       "...      ...                               ...            ...   \n",
       "16496  54196   \"Tales of Light & Dark\" (2013)         horror    \n",
       "16497  54197      Der letzte Mohikaner (1965)        western    \n",
       "16498  54198              Oliver Twink (2007)          adult    \n",
       "16499  54199                Slipstream (1973)          drama    \n",
       "16500  54200        Curitiba Zero Grau (2010)          drama    \n",
       "\n",
       "                                             Description  \n",
       "1       In a make-believe, mittleuropean kingdom, a v...  \n",
       "2       At the end of World War II, Korea was divided...  \n",
       "3       Colombia is coming out of a period in their h...  \n",
       "4       Japanese bombs hit Pearl Harbor on a Sunday. ...  \n",
       "5       Connect with English is a series that brings ...  \n",
       "...                                                  ...  \n",
       "16496   Covering multiple genres, Tales of Light & Da...  \n",
       "16497   As Alice and Cora Munro attempt to find their...  \n",
       "16498   A movie 169 years in the making. Oliver Twist...  \n",
       "16499   Popular, but mysterious rock D.J Mike Mallard...  \n",
       "16500   Curitiba is a city in movement, with rhythms ...  \n",
       "\n",
       "[16500 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e8ac2c0-ad1a-4e77-b8dd-37251e264609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634b3280-12df-47a2-ab17-34c42a8efab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\susha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\susha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2bb886d-3c57-481d-abf6-eb6e6bd1215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Remove special characters, punctuation, and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Apply preprocessing to 'Description' column\n",
    "train_data['Description'] = train_data['Description'].apply(preprocess_text)\n",
    "test_data['Description'] = test_data['Description'].apply(preprocess_text)\n",
    "test_data_solution['Description'] = test_data_solution['Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfc39e69-1300-4e6f-a9be-c5a8616d600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sno                                                        90002\n",
       "Name                                  The Copper Village (2015) \n",
       "Genre                                               documentary \n",
       "Description    villag western nepal copper mine oper 400 year...\n",
       "Name: 35786, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[90000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7585bb6-caf2-4246-8ad4-644bcba12d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['Description']\n",
    "y_train = train_data['Genre']\n",
    "X_test = test_data['Description']\n",
    "y_test_solution = test_data_solution['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e926400-2cf7-4905-b0c7-2cc975aa8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa72b13-f7ab-46ce-8c40-d7ebf50a777d",
   "metadata": {},
   "source": [
    "### Using TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6338a105-f42e-4b27-8736-77d8dc2a18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.14971738 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "X_train_vectorized = vectorizer.fit_transform(X_train).toarray()\n",
    "print(X_train_vectorized)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8cafe56-b48e-4013-a066-8edb14664312",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "head not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35816\\1838096729.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: head not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb3b512d-f49a-4353-ac2d-eb8038018d1c",
   "metadata": {},
   "source": [
    "#### MultiClass Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aecaffdf-8b1b-43a5-b26b-d393f302788d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8860\\1529372824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;31m# Apply preprocessing to 'Description' column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mtest_data_solution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data_solution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4432\u001b[0m         \"\"\"\n\u001b[1;32m-> 4433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8860\\1529372824.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^\\w\\s]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     19\u001b[0m         return [\n\u001b[0;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m    230\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Convert sparse matrices to CSR format\n",
    "# X_train_csr = X_train_vectorized.tocsr()\n",
    "# X_test_csr = X_test_vectorized.tocsr()\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_csr, y_train_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert sparse matrices to COO format for easier manipulation\n",
    "# X_train_coo = X_train.tocoo()\n",
    "# X_val_coo = X_val.tocoo()\n",
    "\n",
    "# # Reshape data to ensure rank-2 tensors\n",
    "# X_train_sparse = tf.sparse.reorder(tf.sparse.SparseTensor(\n",
    "#     indices=np.transpose([X_train_coo.row, X_train_coo.col]),\n",
    "#     values=X_train_coo.data,\n",
    "#     dense_shape=X_train_coo.shape\n",
    "# ))\n",
    "# X_val_sparse = tf.sparse.reorder(tf.sparse.SparseTensor(\n",
    "#     indices=np.transpose([X_val_coo.row, X_val_coo.col]),\n",
    "#     values=X_val_coo.data,\n",
    "#     dense_shape=X_val_coo.shape\n",
    "# ))\n",
    "\n",
    "# # Define and train the model\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_dim=X_train_sparse.shape[1]),\n",
    "#     Dropout(0.8),\n",
    "#     #Dense(128, activation='relu'),\n",
    "\n",
    "#     Dense(128, activation='relu'),\n",
    "#     # Dropout(0.8),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.8),\n",
    "#     Dense(64, activation='relu'),\n",
    "\n",
    "#     Dense(32, activation='relu'),\n",
    "    \n",
    "#     # Dense(32, activation='relu'),\n",
    "#     Dense(27, activation='softmax')  \n",
    "# ])\n",
    "# optimizer = Adam(learning_rate=0.002)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(X_train_sparse, y_train, epochs=30, batch_size=64, validation_data=(X_val_sparse, y_val))\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to 'Description' column\n",
    "train_data['Description'] = train_data['Description'].apply(preprocess_text)\n",
    "test_data['Description'] = test_data['Description'].apply(preprocess_text)\n",
    "test_data_solution['Description'] = test_data_solution['Description'].apply(preprocess_text)\n",
    "\n",
    "# Data preparation\n",
    "X_train = train_data['Description']\n",
    "y_train = train_data['Genre']\n",
    "X_test = test_data['Description']\n",
    "y_test_solution = test_data_solution['Genre']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "X_train_vectorized = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vectorized = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_solution_encoded = label_encoder.transform(y_test_solution)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, y_train_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63533cff-b90c-4923-8257-ffc7046f1944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 2.0303 - accuracy: 0.4134 - val_loss: 1.8231 - val_accuracy: 0.4841\n",
      "Epoch 2/30\n",
      "1149/1149 [==============================] - 16s 14ms/step - loss: 1.8558 - accuracy: 0.4671 - val_loss: 1.7513 - val_accuracy: 0.5072\n",
      "Epoch 3/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.7902 - accuracy: 0.4861 - val_loss: 1.7247 - val_accuracy: 0.5102\n",
      "Epoch 4/30\n",
      "1149/1149 [==============================] - 12s 11ms/step - loss: 1.7475 - accuracy: 0.4977 - val_loss: 1.6819 - val_accuracy: 0.5170\n",
      "Epoch 5/30\n",
      "1149/1149 [==============================] - 12s 10ms/step - loss: 1.7275 - accuracy: 0.5015 - val_loss: 1.6888 - val_accuracy: 0.5155\n",
      "Epoch 6/30\n",
      "1149/1149 [==============================] - 12s 11ms/step - loss: 1.7026 - accuracy: 0.5072 - val_loss: 1.6524 - val_accuracy: 0.5212\n",
      "Epoch 7/30\n",
      "1149/1149 [==============================] - 13s 11ms/step - loss: 1.6899 - accuracy: 0.5133 - val_loss: 1.6671 - val_accuracy: 0.5173\n",
      "Epoch 8/30\n",
      "1149/1149 [==============================] - 16s 14ms/step - loss: 1.6763 - accuracy: 0.5154 - val_loss: 1.6650 - val_accuracy: 0.5179\n",
      "Epoch 9/30\n",
      "1149/1149 [==============================] - 16s 14ms/step - loss: 1.6588 - accuracy: 0.5209 - val_loss: 1.6534 - val_accuracy: 0.5197\n",
      "Epoch 10/30\n",
      "1149/1149 [==============================] - 16s 14ms/step - loss: 1.6515 - accuracy: 0.5241 - val_loss: 1.6437 - val_accuracy: 0.5264\n",
      "Epoch 11/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.6370 - accuracy: 0.5275 - val_loss: 1.6339 - val_accuracy: 0.5267\n",
      "Epoch 12/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.6250 - accuracy: 0.5310 - val_loss: 1.6333 - val_accuracy: 0.5236\n",
      "Epoch 13/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.6184 - accuracy: 0.5321 - val_loss: 1.6280 - val_accuracy: 0.5267\n",
      "Epoch 14/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.6142 - accuracy: 0.5336 - val_loss: 1.6364 - val_accuracy: 0.5246\n",
      "Epoch 15/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.5961 - accuracy: 0.5411 - val_loss: 1.6476 - val_accuracy: 0.5247\n",
      "Epoch 16/30\n",
      "1149/1149 [==============================] - 15s 13ms/step - loss: 1.5927 - accuracy: 0.5413 - val_loss: 1.6311 - val_accuracy: 0.5247\n",
      "Epoch 17/30\n",
      "1149/1149 [==============================] - 15s 13ms/step - loss: 1.5834 - accuracy: 0.5445 - val_loss: 1.6359 - val_accuracy: 0.5268\n",
      "Epoch 18/30\n",
      "1149/1149 [==============================] - 15s 13ms/step - loss: 1.5742 - accuracy: 0.5437 - val_loss: 1.6290 - val_accuracy: 0.5287\n",
      "Epoch 19/30\n",
      "1149/1149 [==============================] - 13s 11ms/step - loss: 1.5732 - accuracy: 0.5480 - val_loss: 1.6269 - val_accuracy: 0.5293\n",
      "Epoch 20/30\n",
      "1149/1149 [==============================] - 13s 12ms/step - loss: 1.5628 - accuracy: 0.5500 - val_loss: 1.6272 - val_accuracy: 0.5269\n",
      "Epoch 21/30\n",
      "1149/1149 [==============================] - 13s 12ms/step - loss: 1.5584 - accuracy: 0.5498 - val_loss: 1.6246 - val_accuracy: 0.5303\n",
      "Epoch 22/30\n",
      "1149/1149 [==============================] - 13s 11ms/step - loss: 1.5539 - accuracy: 0.5510 - val_loss: 1.6381 - val_accuracy: 0.5285\n",
      "Epoch 23/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.5429 - accuracy: 0.5553 - val_loss: 1.6407 - val_accuracy: 0.5253\n",
      "Epoch 24/30\n",
      "1149/1149 [==============================] - 14s 12ms/step - loss: 1.5410 - accuracy: 0.5568 - val_loss: 1.6227 - val_accuracy: 0.5262\n",
      "Epoch 25/30\n",
      " 797/1149 [===================>..........] - ETA: 3s - loss: 1.5332 - accuracy: 0.5592"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8860\\3849034689.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1688\u001b[0m                             \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m                             \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mstep_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1393\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_spe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_increment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;34m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dropout(0.8),\n",
    "    Dense(256, activation='relu'),\n",
    "    # Dropout(0.8),\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout(0.8),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(27, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0035)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "900b691d-b5da-457d-983b-a58d1bdd2d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8860\\3035046171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7d521e4-a18a-46bd-a9a8-80563382c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5322424242424243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susha\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectorized, y_train_encoded)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_solution_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "336c69f7-725c-4a8d-9973-17fba9762430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1479"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = train_data['Description'].apply(lambda x: len(word_tokenize(x))).max()\n",
    "max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a35f8-0fe0-4639-adb1-8b62d457c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4c15a9c-f6e9-4bdc-95c0-f8d68466bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000) \n",
    "\n",
    "# Fit and transform the training and test text data\n",
    "X_train_vectorized = vectorizer.fit_transform(train_data['Description'])\n",
    "X_test_vectorized = vectorizer.transform(test_data['Description'])\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X_train_dense = X_train_vectorized.toarray()\n",
    "X_test_dense = X_test_vectorized.toarray()\n",
    "\n",
    "# Define maximum sequence length based on the maximum number of tokens\n",
    "max_length = 800  \n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_padded = pad_sequences(X_train_dense, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_dense, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626af604-c2aa-4a5c-ace3-d283da2b0dea",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71bf9832-a81a-4f35-8bca-01f34116cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 800, 128)          102400    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 800, 256)         263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 800, 32)           8224      \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 800, 27)           891       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374,683\n",
      "Trainable params: 374,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\susha\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\susha\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\susha\\AppData\\Local\\Temp\\ipykernel_35816\\3013874624.py\", line 18, in <module>\n      history = model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=64, validation_split=0.2)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [51200,27] and labels shape [64]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_2277178]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35816\\3013874624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Evaluate the model on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"C:\\Users\\susha\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\susha\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\susha\\AppData\\Local\\Temp\\ipykernel_35816\\3013874624.py\", line 18, in <module>\n      history = model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=64, validation_split=0.2)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\susha\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [51200,27] and labels shape [64]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_2277178]"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=X_train_padded.shape[1], output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.8),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test_solution_encoded)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429e6a2-c454-46ef-bf63-687a96141ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6dbfb-adf7-4083-aabb-98be5260685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2586/2586 [==============================] - ETA: 0s - loss: 2.3523 - accuracy: 0.2456"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43068633-0281-4293-8119-bd716f979926",
   "metadata": {},
   "source": [
    "### Using Word2Vec vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cdf13dd3-8ccd-440f-b16c-b422fbfa8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97235332-b74a-42e5-8bb4-1e51a4880817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=train_data['Description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to calculate document vectors\n",
    "def calculate_doc_vector(tokens):\n",
    "    vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec_model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9212bc27-3b01-4b16-9c8f-30c453344a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document vectors for train and test data\n",
    "X_train_vectors = np.array([calculate_doc_vector(tokens) for tokens in train_data['Description']])\n",
    "X_test_vectors = np.array([calculate_doc_vector(tokens) for tokens in test_data['Description']])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(train_data['Genre'])\n",
    "y_test_solution_encoded = label_encoder.transform(test_data_solution['Genre'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4766a-bf5e-45c3-b375-324601a7c183",
   "metadata": {},
   "source": [
    "#### MultiClass Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40bf7e91-6961-42e6-ae6a-c1d09db4c7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susha\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectors, y_train_encoded)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf6b8d02-b8f2-49ca-800a-2a87808fd02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3626666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test_solution_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6fbe225-30b5-4a1e-a326-b1670750abd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42fa4be9-c629-4048-a2a6-3c1bb30b9422",
   "metadata": {},
   "source": [
    "####  Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df453534-d53b-45c3-9826-8a9b54402aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1293/1293 [==============================] - 67s 50ms/step - loss: 2.3536 - accuracy: 0.2472 - val_loss: 2.3398 - val_accuracy: 0.2547\n",
      "Epoch 2/15\n",
      "1293/1293 [==============================] - 62s 48ms/step - loss: 2.3352 - accuracy: 0.2676 - val_loss: 2.3103 - val_accuracy: 0.3083\n",
      "Epoch 3/15\n",
      "1293/1293 [==============================] - 60s 46ms/step - loss: 2.2864 - accuracy: 0.3120 - val_loss: 2.2716 - val_accuracy: 0.3176\n",
      "Epoch 4/15\n",
      "1293/1293 [==============================] - 58s 45ms/step - loss: 2.2675 - accuracy: 0.3190 - val_loss: 2.2540 - val_accuracy: 0.3253\n",
      "Epoch 5/15\n",
      "1293/1293 [==============================] - 59s 45ms/step - loss: 2.2691 - accuracy: 0.3178 - val_loss: 2.2596 - val_accuracy: 0.3218\n",
      "Epoch 6/15\n",
      "1293/1293 [==============================] - 59s 45ms/step - loss: 2.2384 - accuracy: 0.3287 - val_loss: 2.2355 - val_accuracy: 0.3329\n",
      "Epoch 7/15\n",
      "1293/1293 [==============================] - 58s 45ms/step - loss: 2.2225 - accuracy: 0.3357 - val_loss: 2.2174 - val_accuracy: 0.3358\n",
      "Epoch 8/15\n",
      "1293/1293 [==============================] - 62s 48ms/step - loss: 2.2103 - accuracy: 0.3421 - val_loss: 2.2153 - val_accuracy: 0.3420\n",
      "Epoch 9/15\n",
      "1293/1293 [==============================] - 59s 45ms/step - loss: 2.2007 - accuracy: 0.3436 - val_loss: 2.1985 - val_accuracy: 0.3438\n",
      "Epoch 10/15\n",
      "1293/1293 [==============================] - 58s 45ms/step - loss: 2.1925 - accuracy: 0.3471 - val_loss: 2.1936 - val_accuracy: 0.3454\n",
      "Epoch 11/15\n",
      "1293/1293 [==============================] - 57s 44ms/step - loss: 2.1872 - accuracy: 0.3490 - val_loss: 2.1895 - val_accuracy: 0.3453\n",
      "Epoch 12/15\n",
      "1293/1293 [==============================] - 57s 44ms/step - loss: 2.1841 - accuracy: 0.3486 - val_loss: 2.1962 - val_accuracy: 0.3449\n",
      "Epoch 13/15\n",
      "1293/1293 [==============================] - 57s 44ms/step - loss: 2.1782 - accuracy: 0.3500 - val_loss: 2.1865 - val_accuracy: 0.3467\n",
      "Epoch 14/15\n",
      "1293/1293 [==============================] - 58s 45ms/step - loss: 2.1741 - accuracy: 0.3508 - val_loss: 2.1889 - val_accuracy: 0.3419\n",
      "Epoch 15/15\n",
      "1293/1293 [==============================] - 58s 45ms/step - loss: 2.1702 - accuracy: 0.3516 - val_loss: 2.1823 - val_accuracy: 0.3476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad2686af10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_reshaped = np.expand_dims(X_train_vectors, axis=-1)\n",
    "X_test_vectors_reshaped = np.expand_dims(X_test_vectors, axis=-1)\n",
    "\n",
    "# Define MLPClassifier\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the optimizer with a custom learning rate\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectors_reshaped, y_train_encoded, epochs=15, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f896781f-40e0-40a9-9f80-071ed45c3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2vec_model.wv), output_dim=100, input_length=900),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),  # Increase units to 128 and return_sequences=True\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),  # Increase units to 64\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),  # Add an additional Dense layer with 64 units and ReLU activation\n",
    "    Dense(32, activation='relu'),  # Add another Dense layer with 32 units and ReLU activation\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1873af3-9262-47e9-9299-dacf63830fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9191400 into shape (301)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32092\\2200893086.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Reshape input data to have shape (None, num_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train_vectors_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m301\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_test_vectors_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 9191400 into shape (301)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Determine the number of features after reshaping\n",
    "# num_features = X_train_vectors.shape[1]  # Only one dimension beyond batch size\n",
    "\n",
    "# Reshape input data to have shape (None, num_features)\n",
    "X_train_vectors_reshaped = X_train_vectors.reshape(-1, 301)\n",
    "X_test_vectors_reshaped = np.expand_dims(X_test_vectors, axis=-1)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the optimizer with a custom learning rate\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectors_reshaped, y_train_encoded, epochs=15, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4c6bd92-8961-4ab6-8c92-0e2888032792",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30608\\1646491257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2vec_model.wv), output_dim=100, input_length=900),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251ad81-80f3-461d-a7c7-eae6e43932bc",
   "metadata": {},
   "source": [
    "# In conclusion word2vec vectorization technique seems to get the best out of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8ed20-43f2-416b-9724-f25e69af8ea9",
   "metadata": {},
   "source": [
    "## Now let's look into some Machine Learning models to see if it generates better accuracy models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c157c-3c3d-4125-8ec7-845ee2d6c5f7",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59b164ad-e913-48c7-9508-4e7fdeec7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec382a2a-3823-4ba9-a5f6-c4b078d7d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34763636363636363\n"
     ]
    }
   ],
   "source": [
    "model = SVC()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectors, y_train_encoded)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "\n",
    "accuracy = accuracy_score(y_test_solution_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34453f0-987f-458a-a763-2e16abf30185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
